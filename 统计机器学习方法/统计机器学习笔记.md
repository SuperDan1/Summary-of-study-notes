---
typora-copy-images-to: pic
---

参考：《统计学习方法》——李航

[TOC]





# k近邻法

* K近邻法是一种基本分类与回归方法，分类时对新的实例，根据其**k个最近邻**的训练实例的类别，通过**多数表决**等方式进行预测
    * 输入为实例的特征向量，对应于特征空间的点
    * 输出为实例的类别
    * 不具有显式的学习过程，实际上利用训练数据集对**特征向量空间**进行**划分**，并作为其分类的模型

* K近邻法的三个基本要素
    * k值的选择
    * 距离度量
    * 分类决策规则

## k近邻算法
* 算法实现
    * 输入：训练数据集$T = \{ ({x_1},{y_1}),({x_2},{y_2}),\cdots,({x_N},{y_N})\}$，其中${x_i} \in x \subseteq {\Re ^n}$为实例的特征向量，${y_i} \in {\cal{Y}} = \{ {c_1},{c_2}, \cdots ,{c_k}\}$为实例的类别，$i=1,2,\cdots,N$
    * 输出：实例$x$所属的类别$y$
    * 根据给定的距离度量，在训练集$T$中找出与x最近邻的k个点，涵盖这k个点的x的邻域记作$N_k(x)$
    * 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$：$y = \mathop {\arg \max }\limits_{{c_j}} \sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$，$i=1,2,\cdots,N;j=1,2,\cdots,K$
    其中$I$是指示函数，即当$y_i=c_j$时$I=1$，否则为0
* k=1时称为最近邻算法

## k近邻模型
* 模型：
    * 特征空间中，对每个训练实例点$x_i$，距离改该点比其他点更近的所有点组成一个区域，叫做单元（cell）
    * 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分
![k近邻](C:\Users\SuperDan\Desktop\机器学习笔记\pic\k近邻法的模型对应特征空间的一个划分.jpg)
k近邻法的模型对应特征空间的一个划分

* 距离度量：特征空间中两个实例点相似程度的反映，假设特征空间x是n维实数向量空间${\Re ^n}$，${x_i},{x_j} \in \chi ,{x_i} = {(x_i^{(1)},x_i^{(2)}, \cdots ,x_i^{(n)})^T},{x_j} = {(x_j^{(1)},x_j^{(2)}, \cdots ,x_j^{(n)})^T}$
    * Lp距离（Lp distance）：${L_p}({x_i},{x_j}) = {\left( {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}^p}} } \right)^{\frac{1}{p}}}$，$p \ge 1$
        * 当p=2时，称为欧式距离，即${L_2}({x_i},{x_j}) = {\left( {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}^2}} } \right)^{\frac{1}{2}}}$
        * 当p=1时，称为曼哈顿（Manhattan distance）：${L_1}({x_i},{x_j}) = { {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}}} } }$
        * 当$p = \infty$，它是各个坐标距离的最大值${L_\infty }({x_i},{x_j}) = \mathop {\max }\limits_i \left| {x_i^{(l)} - x_j^{(l)}} \right|$
![Lp距离间的关系](C:\Users\SuperDan\Desktop\机器学习笔记\pic\Lp距离间的关系.jpg)
上图为二维空间中p取不同值时，与原点的$L_p$距离为1的点的图形
* k值的选择
    * 选择较小的值：相当于用较小的邻域中的训练实例进行预测，k值的减小意味着整体模型变复杂，容易发生**过拟合**
        * 学习的近似误差（approximation error）会减小
        * 学习的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感
    * 选择较大的值：相当于用较大的邻域中的训练实例进行预测，意味着模型变得简单
        * 可以减少学习的估计误差
        * 近似误差会增大
    * k=N：无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类，模型过于简单，完全忽略训练实例中的大量有用信息，不可取
    * 在实际应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值

* 分类决策规则：往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类
    * 如果分类的损失函数为0-1损失函数，分类函数为$f:{\Re ^n} \to \{ {c_1},{c_2}, \cdots ,{c_k}\}$，那么误分类的概率为$P(Y \ne f(X)) = 1 - P(Y = f(X))$
    * 对于给定的实例$x_i\in x$，其最近邻的k个训练实例点构成集合$N_k(x)$；如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是$\frac{1}{k}\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} \ne {c_j})}  = 1 - \frac{1}{k}\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$
    * 要使误分类率最小即经验风险最小，就要使$\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$最大
    * 所以多数表决规则等价于经验风险最小化（损失函数最小化）
## k近邻法的实现：kd树
TODO



  

# 决策树

## 决策树模型与学习

### 决策树模型

* 定义：由**结点**（node）和**有向边**（directed edge）组成
    * 结点
        * **内部结点**：一个**特征**或属性——每个节点包含的样本结合根据属性测试的结果被划分到子节点中
        
        * **叶结点**（下面没有分支的节点）：一个**类别**

      ![c3bcf439902caf782a159fdb60614fc6.png](C:\Users\SuperDan\Desktop\机器学习笔记\pic\决策树.png)
      
        * 决策树学习的**目的**：为了产生一颗**泛化能力强**，即处理未见示例能力强的决策树；
    
    * 从根节点到每个叶节点的路径对应了一个判定测试序列，其基本流程遵循简单且直观地“**分而治之**”（divide-and-conquer）策略
    
* 可以将决策树看成一个**if-then**规则的集合：互斥且完备——每一个实例都被一条路径或一条规则所覆盖，而且只被一条规则所覆盖
  流程图如下所示
![](C:\Users\SuperDan\Desktop\机器学习笔记\pic\决策树流程.png)
  
* 决策树的生成是一个递归过程，有三种情形导致递归返回：
    * 当前节点包含的样本全属于同一类别，无法划分；
    * 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；
    * 当前节点包含的样本集合为空，不能划分；把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别；

### 决策树与条件概率

* 决策树表示给定特征条件下类的条件概率分布——定义在特征空间的一个划分（partition）上
  * 将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布
  * 决策树的一条路径相应于划分中的一个单元
  * 决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成$P(Y|X)$
  * 各叶结点（单元）上的条件概率往往偏向某一个类——属于某一类的概率较大

## 划分选择
* 希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高
    * **信息熵**（information entropy）：度量样本集合纯度最常用的一种指标。假设当前集合D中第k类样本所占的**比例**为$p_k$，则信息熵公式为$Ent(D) =  - \sum\limits_{k = 1}^y {{p_k}{{\log }_2}{p_k}}$，**值越小**，**纯度越高**；
    
    * 经验熵（conditional entropy）：设有随机变量$(X,Y)$，其联合概率分布为
        $$
        P(X = {x_i},Y = {y_i}) = {p_{ij}},i = 1,2, \cdots ,n;j = 1,2, \cdots ,m
        $$
        条件熵表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望
        $$
        H(Y|X) = \sum\limits_{i = 1}^n {{p_i}H(Y|X = {x_i})} 
        $$
        其中，${p_i} = P(X = {x_i}),i = 1,2, \cdots ,n$
    
        当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）
    
    * **信息增益**（information gain）：表征选用不同**属性**作为**划分准则**的信息熵减少的数值，所以**信息增益越大**，意味着使用这个属性进行划分获得的**纯度提升大**；
      
        * 特征$A$对训练数据集$D$的信息增益定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差
            $$
            g(D,A)=H(D)-H(D|A)
            $$
        
        * 基本假设
        
            * 训练数据集为$D$，$|D|$表示其样本容量，即样本个数
              
                * 有$K$个类$C_k,k=1,2,\cdots,K$，$|C_K|$为属于类$C_k$的样本数量，$\sum\limits_{k = 1}^K {|{C_k}|}  = |D|$
            * 假如离散属性$A$有$n$个可能取值$\left\{ {{a^1},{a^2}, \cdots ,{a^n}} \right\}$，若使用$A$来对$\bf{D}$进行划分，将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum\limits_{i= 1}^n {|{D_i}|}  = |D|$
            
            * 子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$
              
            * 计算数据集$D$的经验熵$H(D)$
                $$
                H(D) =  - \sum\limits_{k = 1}^K {\frac{{|{C_k}|}}{{|D|}}{{\log }_2}} \frac{{|{C_k}|}}{{|D|}}
                $$
        
            * 计算特征$A$对数据集$D$的经验熵$H(D|A)$
                $$
                H(D|A) = \sum\limits_{i = 1}^n {\frac{{|{D_i}|}}{{|D|}}H({D_i})}  =  - \sum\limits_{i = 1}^n {\frac{{|{D_i}|}}{{|D|}}\sum\limits_{k = 1}^K {\frac{{|{D_{ik}}|}}{{|{D_i}|}}{{\log }_2}} \frac{{|{D_{ik}}|}}{{|{D_i}|}}} 
                $$
        
            * 计算信息增益
                $$
                g(D,A) = H(D) - H(D|A)
                $$
        
        * 著名的**ID3**（Iterative Dichotomiser，迭代二分器）决策树学习算法就是以信息增益为准则来选择划分属性；
            * 输入：训练数据集$\bf{D}$，特征集$\bf{A}$，阈值$\varepsilon$
            * 输出：决策树$\bf{T}$
            * 若$\bf{D}$中所有实例属于同一类$C_k$，则$\bf{T}$为单结点树，并将类$C_k$作为该结点的类标记，返回$\bf{T}$
            * 若$\bf{A}=\emptyset$，则$\bf{T}$为单结点树，并将$\bf{D}$中实例数最大的类$C_k$作为该结点的类标记，返回$\bf{T}$
            * 否则，计算$\bf{A}$中各特征对$\bf{D}$的信息增益，选择信息增益最大的特征$A_g$
            * 如果$A_g$的信息增益小于阈值$\varepsilon$，则置$\bf{T}$为单结点树，并将$\bf{D}$中实例数最大的类$C_k$作为该结点的类标记，返回$\bf{T}$
            * 否则，对$A_g$的每一可能值$a_i$，将$\bf{D}$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
            * 对第i个子结点，以$D_i$为训练集，以$\bf{A}-\bf{A_g}$为特征集，递归调用，得到子树$T_i$，返回$T_i$
            只有树的**生成**，所以易产生**过拟合**
        
    * **增益率**：
        * 举例说明信息增益的缺点：**倾向于选择有更多取值的属性**
          比如有20个训练实例，有属性a有20个取值，则根据属性a进行划分，将产生20个分支，每个分支下有1个实例，这样分支节点的纯度已达到最大；然而这样的决策树显然不具有泛化能力，无法对新样本进行有效预测；
        * **信息增益对可取值数目较多的属性有所偏好**，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用增益率来选择最优划分属性：${g_R}(D,A) = \frac{{g(D,A)}}{{H(D)}} = \frac{{H(D) - H(D|A)}}{{H(D)}}$
        * 需要注意的是：**增益率准则对可取值数目较少的属性有所偏好**，因此，**C4.5**算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选划分中找出信息增益高于平均水平的属性**，**再**从中**选择增益率最高**的；
    * C4.5的生成算法，用信息增益比来选择特征，其他与ID3算法相同
      
    * 基尼系数（Gini index）：**CART决策树**（Classification and Regression Tree，分类和回归任务都可以使用）使用基尼系数选择划分属性。
      * 基尼值：$Ginin(D) = \sum\limits_{k = 1}^y {\sum\limits_{k' \ne k} {{p_k}{p_{k'}}} }  = 1 - \sum\limits_{k = 1}^y {p_k^2}$
      直观来说，Gini(D)反映了从数据集D中**随机抽取两个样本**，其类别**标记不一致**的**概率**。Gini(D)**越小**，则数据集D的**纯度越高**；
      * 属性a的基尼指数定义为$\sum\limits_{v = 1}^V {\frac{{\left| {{D^v}} \right|}}{{\left| D \right|}}Gini({D^v})}$; 在候选属性集合中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 ${a^*} = \mathop {\arg \min }\limits_{a \in A} Gini\_index(D,a)$

## 剪枝处理
* 剪枝（pruning）是决策树学习算法**避免过拟合**的主要手段。决策树中有时节点划分过程不断重复，造成决策树分支过多，从而导致过拟合。决策树剪枝的基本策略有**预剪枝**（prepruning）和**后剪枝**（postpruning）。
    * 预剪枝是指在决策树生成过程中，对每个节点在划分前**先基于信息增益准则进行估计**，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点，其类别标记为训练样例最多的类别
      - 跟不剪枝树相比，预剪枝使得决策树的很多剪枝都没有展开，这不仅**降低了过拟合**的风险，还显著**减少**了决策树的**训练时间开销和测试时间开销**；
  - 另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却可能导致性能显著提高；预剪枝基于“本质”禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险；
* 后剪枝是先从训练集生成一颗完整的决策树，然后**自底向上**地对非叶节点进行**考察**，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点；如果**不变**，根据奥卡姆剃刀定律（越简单越好）**也要剪枝**（通过预留一部分数据用作验证集进行性能评估）
  
    * 后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，**后剪枝**决策树的**欠拟合风险**很**小**，**泛化性能**往往**优于**预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶节点进行逐一考察，因此其**训练时间开销**比未剪枝决策树和预剪枝决策树都要**大**得多；
    
* 决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现

    * 设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点由$N_t$个样本点，其中$k$类的样本点有$N_{tk},k=1,2,\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha \ge 0$为系数

    * 决策树学习的损失函数可以定义为
        $$
        {C_\alpha }(T) = \sum\limits_{t = 1}^{|T|} {{N_t}{H_t}(T)}  + \alpha |T|
        $$
        其中经验熵为
        $$
        {H_t}(T) =  - \sum\limits_k {\frac{{{N_{tk}}}}{{{N_t}}}\log } \frac{{{N_{tk}}}}{{{N_t}}}
        $$
        在损失函数中，将右端的第一项记为$C(T)$
        $$
        {C_\alpha }(T) = C(T) + \alpha |T|
        $$
        式中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\alpha$控制两者之间的影响

    * 上面定义的损失函数的极小化等价于正则化的极大似然估计

        ![剪枝](C:\Users\SuperDan\Desktop\机器学习笔记\pic\剪枝.jpg)

## CART算法

* 分类与回归树（classification and regression tree，CART）由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归
  * CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法
  * CART假设决策树是二叉树，内部结点特征的取值“是”和“否”
  * 决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布

### CART生成

* CART树的生成就是递归地构建二叉决策树的过程

  * 回归树：平方误差最小化准则
  * 分类树：基尼指数最小化准则

* 回归树的生成

  * 假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$D$

  * 划分输入空间

    * 选择第$j$个变量$x^{(j)}$和它取的值$s$作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域

    $$
    \begin{array}{l}
    {R_1}(j,s) = \{ x|{x^{\{ j\} }} \le s\} \\
    {R_2}(j,s) = \{ x|{x^{\{ j\} }} > s\} 
    \end{array}
    $$

    * 寻找最优切分变量$j$和最优切分点$s$

    $$
    \mathop {\min }\limits_{j,s} [\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{({y_i} - {c_1})}^2}}  + \mathop {\min }\limits_{{c_2}} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{({y_i} - {c_2})}^2}} ]
    $$

    ​	对固定输入变量$j$可以找到最优切分点$s$
    $$
    \begin{array}{l}
    {{\hat c}_1} = ave({y_i}|{x_i} \in {R_1}(j,s))\\
    {{\hat c}_2} = ave({y_i}|{x_i} \in {R_2}(j,s))
    \end{array}
    $$

    * 遍历所有输入变量，找到最优的切分变量$j$和最优切分点构成$(j,s)$对，依次将输入空间划分为两个区域

  * 对每个区域重复上述划分过程，直到满足停止条件为止，这样就生成了一棵回归树——最小二乘回归树（least square regression tree）
    $$
    f(x) = \sum\limits_{m = 1}^M {{{\hat c}_m}I(x \in {R_m})}
    $$

* 分类树的生成

  * 基尼指数

    * 分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为

    $$
    Gini(p) = \sum\limits_{k = 1}^K {{p_k}(1 - {p_k})}  = 1 - \sum\limits_{k = 1}^K {p_k^2} 
    $$

    * 对于给定的样本集合$D$，其基尼指数为
      $$
      Gini(p) = 1 - \sum\limits_{k = 1}^K {{{(\frac{{|{C_k}|}}{{|D|}})}^2}} 
      $$

    * 如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即${D_1} = \{ (x,y) \in D|A(x) = a\} ,{D_2} = D - {D_1}$，则在特征$A$的条件下，集合$D$的基尼指数定义为
      $$
      Gini(D,A) = \frac{{|{D_1}|}}{{|D|}}Gini({D_1}) + \frac{{|{D_2}|}}{{|D|}}Gini({D_2})
      $$
      ![基尼指数](C:\Users\SuperDan\Desktop\机器学习笔记\pic\基尼指数.jpg)

## 连续与缺失值

* 连续值处理：由于连续属性的可取值数目不再有限，需要**连续属性离散化**；C4.5决策树算法采用的就是**二分法**（bi-partition）对连续属性进行处理。
    * 给定样本集${\bf{D}}$和连续属性a，假定a在${\bf{D}}$上出现了n个不同的取值，将这些值从小到大进行**排序**，记为$\left\{ {{a^1},{a^2}, \cdots ,{a^n}} \right\}$。基于划分点t将D**分为子集**${\bf{D}}_t^ -$和${\bf{D}}_t^ +$，其中${\bf{D}}_t^ -$包含那些在属性a上取值不大于t的样本，而${\bf{D}}_t^ +$则包含在属性a上取值大于t的样本。对于相邻的属性取值$a^i$与$a^{i+1}$来说，t在两者之间取任意值所产生的划分结果相同。所以对连续属性a（有n个值）可考察**n-1个元素**的**候选划分点集合**
${T_a} = \{ \frac{{{a^i} + {a^{i + 1}}}}{2}|1 \le i \le n - 1\}$
候选划分点为两个**相邻值**之间的**中位值**；之后比较候选划分点的信息增益，选择最佳划分点；
* **缺失值处理**：现实任务中会遇到许多不完整样本，即样本的某些属性值缺失。如果简单地放弃不完整样本，仅适用无缺失值的样本来进行学习，显然是对数据信息极大的浪费；需要解决两个问题：
    * 如何在属性值缺失的情况下进行划分属性选择
        * 给定数据集$\bf{D}$和属性a，令$\mathop D\limits^\sim$表示$\bf{D}$中在属性a上**没有缺失值**的样本子集，可以仅根据$\mathop D\limits^\sim$判断属性a的**优劣**
        * 假定属性a上有V个可取值$\mathop {{D^v}}\limits^\sim$表示$\mathop D\limits^\sim$中在属性a上取值为$a^v$的样本子集，将$\mathop {{D_k}}\limits^\sim$表示$\mathop D\limits^\sim$中属于第k类的样本子集
            * 显然有$\mathop D\limits^\sim  =  \cup _{k = 1}^y\mathop {{D^k}}\limits^\sim  =  \cup _{v = 1}^V\mathop {{D^v}}\limits^\sim$
            * 假定我们为每个样本$\bf{x}$赋予一个权值$\omega_x$
                * 无缺失值样本所占的比例：$\rho  = \frac{{\sum\limits_{x \in \mathop D\limits^\sim } {{\omega _x}} }}{{\sum\limits_{x \in D} {{\omega _x}} }}$
                * 无缺失值样本中第k类所占的样本：$\mathop {{p_k}}\limits^\sim  = \frac{{\sum\nolimits_{x \in \mathop {{D_k}}\limits^\sim } {{\omega _x}} }}{{\sum\nolimits_{x \in \mathop D\limits^\sim } {{\omega _x}} }}$
                * 无缺失值样本中在属性a上取值$a^v$的样本所占的比例：$\mathop {{\gamma _v}}\limits^\sim  = \frac{{\sum\nolimits_{x \in \mathop {{D^v}}\limits^\sim } {{\omega _x}} }}{{\sum\nolimits_{x \in \mathop D\limits^\sim } {{\omega _x}} }}$
            * 将上述的信息增益计算式推广为：$Gain(D,a) = \rho *(Ent(\mathop D\limits^\sim) - \sum\limits_{v = 1}^V {\mathop {{\gamma _v}}\limits^\sim Ent(\mathop {{D^v}}\limits^\sim )} )$
            其中，$Ent(\mathop D\limits^\sim ) =  - \sum\limits_{k = 1}^y {\mathop {{p_k}}\limits^\sim {{\log }_2}} \mathop {{p_k}}\limits^\sim$
    * 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分
        * 若样本$\bf{x}$在划分属性a上取值已知，则将其划入与其取值对应的子结点，且样本权值在子结点中保持为$\omega_x$
        * 若样本$\bf{x}$在划分属性a上的取值未知，则将其划入所有子结点，且样本权值在与属性值$a^v$对应的子结点中调整为$\mathop {{\gamma _v} \cdot }\limits^\sim {\omega _x}$；直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去 



