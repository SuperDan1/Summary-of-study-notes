# k近邻法

* K近邻法是一种基本分类与回归方法，分类时对新的实例，根据其**k个最近邻**的训练实例的类别，通过**多数表决**等方式进行预测
  * 输入为实例的特征向量，对应于特征空间的点
  * 输出为实例的类别
  * 不具有显式的学习过程，实际上利用训练数据集对**特征向量空间**进行**划分**，并作为其分类的模型

* K近邻法的三个基本要素
  * k值的选择
  * 距离度量
  * 分类决策规则

## k近邻算法

* 算法实现
  * 输入：训练数据集$T = \{ ({x_1},{y_1}),({x_2},{y_2}),\cdots,({x_N},{y_N})\}$，其中${x_i} \in x \subseteq {\Re ^n}$为实例的特征向量，${y_i} \in {\cal{Y}} = \{ {c_1},{c_2}, \cdots ,{c_k}\}$为实例的类别，$i=1,2,\cdots,N$
  * 输出：实例$x$所属的类别$y$
  * 根据给定的距离度量，在训练集$T$中找出与x最近邻的k个点，涵盖这k个点的x的邻域记作$N_k(x)$
  * 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$：$y = \mathop {\arg \max }\limits_{{c_j}} \sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$，$i=1,2,\cdots,N;j=1,2,\cdots,K$
    其中$I$是指示函数，即当$y_i=c_j$时$I=1$，否则为0
* k=1时称为最近邻算法

## k近邻模型

* 模型：
  * 特征空间中，对每个训练实例点$x_i$，距离改该点比其他点更近的所有点组成一个区域，叫做单元（cell）
  * 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分
    ![k近邻](pic\k近邻法的模型对应特征空间的一个划分.jpg)
    k近邻法的模型对应特征空间的一个划分

* 距离度量：特征空间中两个实例点相似程度的反映，假设特征空间x是n维实数向量空间${\Re ^n}$，${x_i},{x_j} \in \chi ,{x_i} = {(x_i^{(1)},x_i^{(2)}, \cdots ,x_i^{(n)})^T},{x_j} = {(x_j^{(1)},x_j^{(2)}, \cdots ,x_j^{(n)})^T}$
  * Lp距离（Lp distance）：${L_p}({x_i},{x_j}) = {\left( {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}^p}} } \right)^{\frac{1}{p}}}$，$p \ge 1$
    * 当p=2时，称为欧式距离，即${L_2}({x_i},{x_j}) = {\left( {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}^2}} } \right)^{\frac{1}{2}}}$
    * 当p=1时，称为曼哈顿（Manhattan distance）：${L_1}({x_i},{x_j}) = { {\sum\limits_{l = 1}^n {{{\left| {x_i^{(l)} - x_j^{(l)}} \right|}}} } }$
    * 当$p = \infty$，它是各个坐标距离的最大值${L_\infty }({x_i},{x_j}) = \mathop {\max }\limits_i \left| {x_i^{(l)} - x_j^{(l)}} \right|$
      ![Lp距离间的关系](pic\Lp距离间的关系.jpg)
      上图为二维空间中p取不同值时，与原点的$L_p$距离为1的点的图形
* k值的选择
  * 选择较小的值：相当于用较小的邻域中的训练实例进行预测，k值的减小意味着整体模型变复杂，容易发生**过拟合**
    * 学习的近似误差（approximation error）会减小
    * 学习的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感
  * 选择较大的值：相当于用较大的邻域中的训练实例进行预测，意味着模型变得简单
    * 可以减少学习的估计误差
    * 近似误差会增大
  * k=N：无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类，模型过于简单，完全忽略训练实例中的大量有用信息，不可取
  * 在实际应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值

* 分类决策规则：往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类
  * 如果分类的损失函数为0-1损失函数，分类函数为$f:{\Re ^n} \to \{ {c_1},{c_2}, \cdots ,{c_k}\}$，那么误分类的概率为$P(Y \ne f(X)) = 1 - P(Y = f(X))$
  * 对于给定的实例$x_i\in x$，其最近邻的k个训练实例点构成集合$N_k(x)$；如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是$\frac{1}{k}\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} \ne {c_j})}  = 1 - \frac{1}{k}\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$
  * 要使误分类率最小即经验风险最小，就要使$\sum\limits_{{x_i} \in {N_k}(x)} {I({y_i} = {c_j})}$最大
  * 所以多数表决规则等价于经验风险最小化（损失函数最小化）

## k近邻法的实现：kd树

*TODO*

