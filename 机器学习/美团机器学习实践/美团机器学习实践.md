[TOC]

# 问题建模

* 机器学习解决问题的通用流程
  * 问题建模：将具体问题抽象成机器可预测的问题
    * 根据预测目标选择适当的评估指标用于模型评估
      * 明确业务指标
      * 明确模型预测目标
    * 从原始数据中选择最相关的样本子集用于模型训练
      * 数据筛选
      * 数据清洗
    * 划分训练集和测试集——用交叉验证的方法对模型进行选择和评估
  * 特征工程
  * 模型选择
  * 模型融合：充分利用不同模型的差异，进一步优化目标

## 评估指标

### 分类指标

> 找到与线上业务指标一致的线下指标

* 精确率：TP/(TP+FP)
  * 精确率是一个二分类指标，而准确率能应用于多分类
* 召回率：TP/(TP+FN)
* P-R曲线：召回率为横轴，精确率为纵轴
* F1：精确率和召回率的调和平均值 2/F1 = 1/P+1/R
* ROC：很多模型输出是预测概率，使用精确率、召回率这类指标增加了一个超参数——分类阈值，影响模型的泛化能力
  * 纵坐标是真正率TPR（TP/TP+FN），横坐标是假正率FPR（FP/FP+TN）
  * 对所有样本按预测概率排序，以每条样本的预测概率为阈值（先从概率最大的开始，对应ROC曲线的零点），计算对应的FPR和TPR
* AUC：主要与排序有关，对排序敏感，而对预测分数没那么敏感
  * ROC曲线下的面积，取值越大说明模型越可能将正样本排在负样本前面
  * 随机挑选一个正样本和负样本时，分类器将正样本排前面的概率
* 对数损失：-log(Y|X)
  * 对预测概率的似然估计
  * 本质上是利用样本中的已知分布，求解导致这种分布的最佳模型参数，使这种分布出现概率最大
  * logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好；与AUC不同，logloss对预测概率敏感

### 回归指标

* 平均绝对误差（MAE）：L1范数损失
  * 很好地刻画了预测值与真实值的偏差
  * 相当于对数据分布的中值进行拟合
  * XGBoost必须要求损失函数有二阶导数，所以不能直接优化MAE
* 均方根误差（RMSE）
  * 表示预测值和真实值差值的样本标准差
  * 和MAE比，RMSE对大误差样本有更大的惩罚；但它也对离群点敏感，其健壮性不如MAE
  * 相当于对数据分布的平均值进行拟合

### 排序指标

## 样本选择

> 从海量数据中识别和选择相关性高的数据作为机器学习模型输入

* 当数据量过大时，程序有时候会耗费大量计算资源和计算时间，有时候甚至不能正常运行

* 全部数据集包含丰富的信息，相关系太低的数据对解决特定问题可能没有帮助

### 数据去噪

* 去除噪声提高训练集的数据质量，改善模型效果
  * 噪声的存在会导致数据质量变低，影响模型的效果：主要检测和去除训练数据中标注带噪声的实例
  * 在训练集中引入噪声数据也能起到提升模型健壮性的作用：主要是特征缺失的噪声
* 去除误标注实例
  * 集成过滤法
  * 交叉验证委员会过滤法
  * 迭代分割过滤法

### 采样

> 采样是一个完善的统计技术，从整体选择一部分进行推论
>
> * 采样能够克服高维特征以及大量数据导致的问题，有助于降低成本，缩短时间甚至提升效果
> * 在不平衡分类问题中还能帮助平衡样本比例

* 一个好的样本子集应该具有无偏性和很小的样本方差
  * 无偏性：样本期望等于全体样本期望
  * 样本方差是衡量样本估计值和真实值的偏差
* 采样方法
  * 无放回简单随机抽样
  * 有放回简单抽样
  * 平衡采样
  * 分层采样

### 原型选择和训练集选择

## 交叉验证

> 测试样本是用于测试模型对新样本的学习能力，所以在假设测试数据和真实数据是独立同分布的前提下，测试误差可以作为泛化误差的近似

* 留出法（Hold-Out）：随机将数据集划分成互斥的两份，一份为训练集，另一份为测试集
* K折交叉验证：将数据集划分成K份互斥数据集，每次用一份数据测试，其余K-1份数据训练，需要迭代K轮得到K个模型，最后将K份测试结果汇总到一起评估
* 自助法（Boostrapping）：使用有放回的重复采样的方式进行训练集、测试集构建
  * 数据量很大的时候，有36.8%的样本不会被选中，作为测试集
* 总结
  * 留出法和K折交叉验证法在训练模型时用的数据只是整个数据集的一个子集，得到的模型会因为训练集大小不一致产生一定的偏差
  * 自助法能够更好地解决上述问题，但是自助法改变了初始数据集的分布，引入了估计偏差

# 特征工程

## 特征提取

## 特征选择

## 特征处理

* 特征归一化目的
  * 消除数据特征之间的量纲影响，需要对特征进行归一化，使得不同指标之间具有可比性
  * 加快梯度下降收敛速度
* 数值类型归一化
  * 线性函数归一化：将结构映射到$[0,1]$间
  * 标准化：映射到均值为0，标准差为1的分布上
* 类别型特征
  * 序号编码
  * 独热编码

## 特征组合



# 模型评估

## 过拟合与欠拟合

* 过拟合
  * 表现：模型在训练集上的表现很好，但在测试集和新数据上的表现较差
  * 降低过拟合风险的方法
    * 获得更多的训练数据
    * 降低模型复杂度
      * 神经网络中减少网络层数、神经元个数
      * 决策树模型中降低树的深度、进行剪纸等
  * 正则化
  * 集成学习方法
* 欠拟合
  * 表现模型在训练集和预测时表现都不好
  * 降低欠拟合风险的方法
    * 添加新特征
    * 增加模型复杂度
    * 减少正则化系数

# 降维

## 主成分分析——PCA

* 信号具有较大方差：最大化投影方向
* 无监督

## 线性判别分析——LDA

* 监督算法
  * 最大化类间距离
  * 最小化类内距离

# 常用模型

## 逻辑斯蒂回归

> 逻辑斯蒂回归是一种广义线性模型，通过对数概率函数将线性函数的结果进行映射，目标函数的取值从$( - \infty , + \infty )$映射到$(0,1)$

* 逻辑斯蒂回归与线性回归
  * 线性回归在训练时对异常点更加敏感
  * 单位阶跃函数可以将线性回归进行分类，但是不连续并且不充分光滑
  * `sigmoid`单调可微，可以代替阶跃函数
* 梯度下降进行求解
* 为了提高算法收敛速度和节省内存，使用LBFGS、信赖域算法——基于批量处理，无法高效处理超大规模数据集，也无法对线上模型进行快速实时更新
* FTRL
* L1正则逻辑斯蒂回归（Lasso）假设模型参数取值满足拉普拉斯分布
* L2正则逻辑斯蒂（Ridge）假设模型参数取值满足高斯分布

## 场感知因子分解机

## 支持向量机

## 梯度提升树

# 模型融合

# 非监督学习

## K-Means

